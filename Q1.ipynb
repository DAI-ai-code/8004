{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d6d9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "923b40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9406605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "housing_schema = StructType([\n",
    "    StructField('longitude', DoubleType(), True),\n",
    "    StructField('latitude', DoubleType(), True),\n",
    "    StructField('housing_median_age', DoubleType(), True),\n",
    "    StructField('total_rooms', DoubleType(), True),\n",
    "    StructField('total_bedrooms', DoubleType(), True),\n",
    "    StructField('population', DoubleType(), True),\n",
    "    StructField('households', DoubleType(), True),\n",
    "    StructField('median_income', DoubleType(), True),\n",
    "    StructField('median_house_value', DoubleType(), True),\n",
    "    StructField('ocean_proximity', StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cad05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = spark.read.format('csv').options(header=True).load('file:///home/talentum/housing/housing.csv', schema=housing_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1afe4454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f9654",
   "metadata": {},
   "source": [
    "# Question 1 part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "528fdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = housing_df.groupBy('ocean_proximity').avg('median_house_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5de0e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+\n",
      "|ocean_proximity|avg(median_house_value)|\n",
      "+---------------+-----------------------+\n",
      "|         ISLAND|               380440.0|\n",
      "|     NEAR OCEAN|     249433.97742663656|\n",
      "|       NEAR BAY|     259212.31179039303|\n",
      "|      <1H OCEAN|     240084.28546409807|\n",
      "|         INLAND|     124805.39200122119|\n",
      "+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da67e5ea",
   "metadata": {},
   "source": [
    "# Question 1 part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "544387fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|ocean_proximity|     average_value|\n",
      "+---------------+------------------+\n",
      "|         ISLAND|          380440.0|\n",
      "|     NEAR OCEAN|249433.97742663656|\n",
      "|       NEAR BAY|259212.31179039303|\n",
      "|      <1H OCEAN|240084.28546409807|\n",
      "|         INLAND|124805.39200122119|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Initialize (using getOrCreate is safer)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 2. Make sure the DataFrame variable name is correct!\n",
    "# If your data is in 'df', use df. If it's in 'housing_df', use that.\n",
    "housing_df.createOrReplaceTempView('housing_table')\n",
    "\n",
    "# 3. Run the SQL\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ocean_proximity, \n",
    "        AVG(median_house_value) AS average_value\n",
    "    FROM \n",
    "        housing_table\n",
    "    GROUP BY \n",
    "        ocean_proximity\n",
    "\"\"\")\n",
    "\n",
    "# 4. Show the results\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9adc2",
   "metadata": {},
   "source": [
    "# Question 1 part C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b10f9d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Using cached https://files.pythonhosted.org/packages/4a/db/694fd552295ed091e7418d02b6268ee36092d4c93211136c448fe061fe32/kafka_python-2.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b359762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record published successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# 1. Define the connection to the Kafka Broker\n",
    "# Change 'localhost:9092' to your actual broker address\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# 2. Prepare the new housing record\n",
    "new_record = {\n",
    "    \"longitude\": -122.23,\n",
    "    \"latitude\": 37.88,\n",
    "    \"housing_median_age\": 41.0,\n",
    "    \"total_rooms\": 880.0,\n",
    "    \"total_bedrooms\": 129.0,\n",
    "    \"population\": 322.0,\n",
    "    \"households\": 126.0,\n",
    "    \"median_income\": 8.3252,\n",
    "    \"median_house_value\": 452600.0,\n",
    "    \"ocean_proximity\": \"NEAR BAY\"\n",
    "}\n",
    "\n",
    "# 3. Publish (send) the record to the topic 'housing_topic'\n",
    "try:\n",
    "    producer.send('housing_topic', value=new_record)\n",
    "    \n",
    "    # 4. Flush the producer to ensure the message is delivered\n",
    "    producer.flush()\n",
    "    print(\"Record published successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error publishing record: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d2bf7",
   "metadata": {},
   "source": [
    "# Question 1 part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8114b741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for messages (will timeout in 5 seconds if empty)...\n",
      "Record 1: NEAR BAY\n",
      "Record 2: NEAR BAY\n",
      "Record 3: NEAR BAY\n",
      "Finished! Processed 3 total.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.coordinator.heartbeat:Consumer poll timeout has expired. This means the time between subsequent calls to poll() was longer than the configured max_poll_interval_ms, which typically implies that the poll loop is spending too much time processing messages. You can address this either by increasing max_poll_interval_ms or by reducing the maximum size of batches returned in poll() with max_poll_records.\n",
      "WARNING:kafka.coordinator.heartbeat:Consumer poll timeout has expired. This means the time between subsequent calls to poll() was longer than the configured max_poll_interval_ms, which typically implies that the poll loop is spending too much time processing messages. You can address this either by increasing max_poll_interval_ms or by reducing the maximum size of batches returned in poll() with max_poll_records.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from kafka import KafkaConsumer\n",
    "\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'housing_topic',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',\n",
    "    group_id='housing_debug_group',\n",
    "    # Stop waiting if no message arrives for 5 seconds\n",
    "    consumer_timeout_ms=5000, \n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8')) if x else None\n",
    ")\n",
    "\n",
    "print(\"Checking for messages (will timeout in 5 seconds if empty)...\")\n",
    "\n",
    "total_records = 0\n",
    "try:\n",
    "    for message in consumer:\n",
    "        if message.value:\n",
    "            total_records += 1\n",
    "            print(f\"Record {total_records}: {message.value.get('ocean_proximity')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "if total_records == 0:\n",
    "    print(\"No messages found. Is the Producer running?\")\n",
    "else:\n",
    "    print(f\"Finished! Processed {total_records} total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eef766",
   "metadata": {},
   "source": [
    "# Question 1 part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2d50a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Insertion...\n",
      "Iteration 1: Record inserted successfully.\n",
      "Current insertion count: 1\n",
      "Iteration 2: Record inserted successfully.\n",
      "Current insertion count: 2\n",
      "Iteration 3: Record inserted successfully.\n",
      "Current insertion count: 3\n",
      "\n",
      "All 3 records have been published.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# 1. Setup the Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# 2. Define 3 new records to insert\n",
    "new_records = [\n",
    "    {\n",
    "        \"longitude\": -122.25, \"latitude\": 37.85, \"housing_median_age\": 52.0, \n",
    "        \"total_rooms\": 1274.0, \"total_bedrooms\": 235.0, \"population\": 558.0, \n",
    "        \"households\": 219.0, \"median_income\": 5.6431, \"median_house_value\": 341300.0, \n",
    "        \"ocean_proximity\": \"NEAR BAY\"\n",
    "    },\n",
    "    {\n",
    "        \"longitude\": -118.35, \"latitude\": 34.05, \"housing_median_age\": 30.0, \n",
    "        \"total_rooms\": 2500.0, \"total_bedrooms\": 500.0, \"population\": 1200.0, \n",
    "        \"households\": 450.0, \"median_income\": 4.1500, \"median_house_value\": 250000.0, \n",
    "        \"ocean_proximity\": \"<1H OCEAN\"\n",
    "    },\n",
    "    {\n",
    "        \"longitude\": -114.50, \"latitude\": 33.60, \"housing_median_age\": 15.0, \n",
    "        \"total_rooms\": 1500.0, \"total_bedrooms\": 300.0, \"population\": 800.0, \n",
    "        \"households\": 280.0, \"median_income\": 2.5000, \"median_house_value\": 100000.0, \n",
    "        \"ocean_proximity\": \"INLAND\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Starting Insertion...\")\n",
    "\n",
    "# 3. Loop through records and publish\n",
    "for i, record in enumerate(new_records, 1):\n",
    "    producer.send('housing_topic', value=record)\n",
    "    \n",
    "    # Force the message to be sent immediately\n",
    "    producer.flush() \n",
    "    \n",
    "    print(f\"Iteration {i}: Record inserted successfully.\")\n",
    "    print(f\"Current insertion count: {i}\")\n",
    "    \n",
    "    # Small delay to simulate real-time stream\n",
    "    time.sleep(1) \n",
    "\n",
    "print(\"\\nAll 3 records have been published.\")\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a332d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
